<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Evaluation of Visual Encoders for Offline Goal-Conditioned Reinforcement Learning and Behavioral Cloning in OGBench</title>

  <!-- Inter + JetBrains Mono -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=JetBrains+Mono:wght@300;400;500&display=swap" rel="stylesheet">

  <style>
    :root {
      --bg: #fafafa;
      --text: #111;
      --muted: #666;
      --code-bg: #f3f3f3;
      --max-width: 760px;
    }

    body {
      margin: 0;
      padding: 0;
      background: var(--bg);
      color: var(--text);
      font-family: "Inter", sans-serif;
      line-height: 1.65;
      font-size: 16px;
      -webkit-font-smoothing: antialiased;
    }

    .wrapper {
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 2rem 1.5rem 6rem;
    }

    header {
      margin-bottom: 3.5rem;
    }

    header h1 {
      font-family: "JetBrains Mono", monospace;
      font-size: 2rem;
      font-weight: 500;
      margin: 0 0 1rem;
    }

    header p {
      margin: 0;
      color: var(--muted);
      font-family: "JetBrains Mono", monospace;
      font-size: 0.9rem;
    }

    article {
      margin-bottom: 4rem;
    }

    article h2 {
      font-family: "JetBrains Mono", monospace;
      margin-top: 2.5rem;
      margin-bottom: 1rem;
      font-size: 1.5rem;
      font-weight: 500;
    }

    article h3 {
      font-family: "JetBrains Mono", monospace;
      margin-top: 2rem;
      margin-bottom: 0.75rem;
      font-size: 1.1rem;
      font-weight: 500;
    }

    article h4 {
      font-family: "JetBrains Mono", monospace;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
      font-size: 1rem;
      font-weight: 500;
    }

    article p {
      margin-bottom: 1.2rem;
    }

    article ul {
      margin: 0 0 1.2rem 1.2rem;
      padding-left: 1rem;
    }

    code {
      font-family: "JetBrains Mono", monospace;
      background: var(--code-bg);
      padding: 2px 5px;
      border-radius: 4px;
      font-size: 0.95rem;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      font-family: "JetBrains Mono", monospace;
      font-size: 0.95rem;
    }

    th, td {
      padding: 8px;
    }

    thead tr {
      border-bottom: 2px solid #ccc;
    }

    tbody tr {
      border-bottom: 1px solid #eee;
    }

    footer {
      margin-top: 4rem;
      padding-top: 2rem;
      text-align: center;
      font-family: "JetBrains Mono", monospace;
      font-size: 0.8rem;
      color: var(--muted);
    }

    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0f0f0f;
        --text: #eee;
        --muted: #aaa;
        --code-bg: #1b1b1b;
      }
    }
  </style>

  <!-- MathJax for LaTeX rendering -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(", "\\)"]],
        displayMath: [["\\[", "\\]"]]
      }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
  <div class="wrapper">
    <header>
      <h1>Evaluation of Visual Encoders for Offline Goal-Conditioned Reinforcement Learning and Behavioral Cloning in OGBench</h1>
      <p>Published: Dec. 9, 2025</p>
      <p>Authors: Yash Prabhu<sup>*</sup> (<a href="mailto:yprabhu@mit.edu">yprabhu@mit.edu</a>), Richard Lian<sup>*</sup> (<a href="mailto:rlian@mit.edu">rlian@mit.edu</a>)</p>
      <p><sup>*</sup> denotes equal contribution</p>
    </header>

    <article>
      <h2>Motivations</h2>
      <p>
        Over the past decade, the landscape of artificial intelligence has changed almost beyond recognition.
        The modern era of deep learning was catalyzed in 2012 by AlexNet [1], a convolutional
        network that dramatically outperformed traditional pipelines on the ImageNet benchmark. AlexNet did more than
        win a competition: it provided a concrete demonstration that large, deep neural networks could be trained at
        scale and deployed on real-world perception problems. In the years that followed, deep learning rapidly
        expanded through architectures such as LSTMs and RNNs for sequence modeling, Transformers for attention-based
        reasoning, and generative models such as diffusion and flow-based models for high-fidelity synthesis.
      </p>

      <p>
        These architectural innovations, combined with large datasets and powerful hardware, culminated in today’s
        foundation models. Large language models like ChatGPT, Gemini, and Claude exhibit strikingly general capabilities:
        they can summarize, translate, write code, reason about instructions, and sometimes even plan at a high level.
        It is tempting to view these systems as the first step toward universal “world models” that can flexibly adapt
        to new tasks given only minimal prompting.
      </p>

      <p>
        However, the success of foundation models has been concentrated in digital and symbolic domains. A parallel
        effort in the research community and industry aims to bring comparable generality to <em>embodied</em> systems—
        robots and agents that perceive and act in the physical world. Startups and labs such as Physical Intelligence,
        SKILD, Figure, and others share a common dream: leverage large-scale representation learning and reinforcement
        learning (RL) to build general-purpose control systems that can manipulate objects, navigate complex
        environments, and adapt to new tasks with relatively little additional training.
      </p>

      <p>
        Realizing this vision requires bridging a substantial gap between representation learning and control.
        Perception-only world models, such as those learned by self-supervised visual encoders, are not sufficient on
        their own: an embodied agent must transform high-dimensional sensory input (e.g., images) into temporally
        coherent, goal-directed actions under uncertainty. Reinforcement learning provides a natural mathematical
        framework for this transformation. In RL, an agent interacts with an environment, receives observations, selects
        actions, and is guided by scalar rewards that encode task performance. RL has produced impressive results in
        locomotion, manipulation, and navigation, especially in simulation. Yet classical RL typically assumes
        low-dimensional state representations, carefully engineered rewards, and extensive online interaction—assumptions
        that are difficult to satisfy in real-world robotics.
      </p>

      <p>
        Goal-Conditioned Reinforcement Learning (GCRL) relaxes some of these constraints by treating the desired outcome
        of an episode as an explicit input to the policy. Rather than learning a single task-specific behavior, the agent
        aims to reach arbitrary goal states \(g\), making it possible to train a <em>family</em> of behaviors
        parameterized by the goal. When combined with offline learning and behavioral cloning, GCRL can be trained
        entirely from static datasets of trajectories, without requiring online access to the environment. This is
        particularly appealing for robotics, where data collection is expensive and risky.
      </p>

      <p>
        In vision-based GCRL, the agent’s observations and goals are raw images. The quality of behavior is therefore
        tightly coupled to the choice of visual encoder that maps pixels to latent representations. Historically, many
        RL pipelines have relied on relatively shallow convolutional networks such as IMPALA, which are easy to train
        from scratch and provide strong locality biases. At the same time, modern vision research has produced a wide
        range of powerful encoders, including deep ResNets, Vision Transformers (ViT), and self-supervised models like
        DINOv3 [2]. These models achieve impressive performance on classification, segmentation, and retrieval, but it
        remains unclear whether—and how—their representations transfer to offline goal-conditioned control.
      </p>

      <p>
        This motivates the central question of our study:
        <br><br>
        <strong>How does the choice of visual encoder affect the performance of offline goal-conditioned reinforcement
        learning and behavioral cloning on pixel-based tasks?</strong>
        <br><br>
        To answer this question, we turn to OGBench [3], a benchmark that standardizes offline GCRL evaluation across dozens
        of environments. OGBench uses an IMPALA-style encoder by default, effectively fixing the representation and
        varying only the algorithm. In contrast, we fix the algorithm to goal-conditioned behavioral cloning (GCBC) and
        systematically vary the visual encoder.
      </p>

      <p>
        Concretely, we compare four encoders—IMPALA, an RL-adapted ResNet50, a Vision Transformer (ViT-Small) trained
        from scratch, and a frozen DINOv3-S/16 backbone—on two visual OGBench tasks: a cube manipulation environment
        and an AntMaze navigation task. Our experiments show that these encoders exhibit complementary strengths and
        weaknesses, and that blindly importing state-of-the-art vision models into offline RL pipelines often fails to
        yield the expected gains. The rest of this article formalizes the goal-conditioned setting, surveys related work,
        describes our methods at a high level, presents empirical results, and concludes with a discussion of open
        questions. Detailed architecture specifications, training hyperparameters, loss curves, representation analyses,
        and environment settings are provided in the Appendix.
      </p>

      <h2>Related Works</h2>

      <h3>Goal-Conditioned Reinforcement Learning</h3>
      <p>
        Goal-conditioned reinforcement learning (GCRL) extends the standard RL formulation by conditioning the policy
        and value functions on a goal state. We consider a controlled Markov process defined by state space
        \( \mathcal{S} \), action space \( \mathcal{A} \), initial state distribution \( \mu(s) \), and transition
        dynamics \( p(s' \mid s, a) \). In contrast to reward-centric formulations, we assume the environment dynamics
        are given but do not rely on an externally specified reward function during training.
      </p>

      <p>
        In the offline setting, the agent observes a static dataset of trajectories:
      </p>

      <p style="text-align:center;">
        \( \mathcal{D} = \left\{ (s_0^{(n)}, a_0^{(n)}, s_1^{(n)}, \ldots, s_{T_n}^{(n)}) \right\}_{n=1}^{N}, \)
      </p>

      <p>
        collected by one or more behavior policies. The agent cannot gather additional experience. The objective is to
        learn a goal-conditioned policy \( \pi(a \mid s, g) \) that can reliably reach a target state \( g \in
        \mathcal{S} \) from diverse starting states. A common formalization is to maximize the expected discounted
        probability of hitting the goal:
      </p>

      <div style="text-align:center; margin: 1em 0;">
        \[
          \mathbb{E}_{\tau \sim p(\tau \mid g)}
          \left[ \sum_{t=0}^{T} \gamma^{t} \mathbf{1}[s_t = g] \right],
        \]
      </div>

      <p>
        where \( \gamma \in (0,1) \) is a discount factor and \( p(\tau \mid g) \) is the trajectory distribution
        induced by conditioning on goal \(g\).
      </p>

      <p>
        In practice, directly optimizing this objective is difficult, and many offline GCRL methods make use of goal
        relabeling or hindsight techniques. Goal-conditioned behavioral cloning (GCBC), which is the agent of choice in this work,
        follows a particularly simple strategy: treat future states in a trajectory as goals for earlier states, and
        train a supervised policy to imitate the observed actions given these goals. Despite its simplicity, GCBC has
        proven surprisingly effective in offline settings, especially when combined with large and diverse datasets.
      </p>

      <h3>OGBench</h3>
      <p>
        OGBench [3] is a benchmark suite designed to evaluate offline goal-conditioned RL methods across a diverse set of
        tasks. It includes both low-dimensional state-based environments and higher-dimensional visual tasks, covering
        navigation, manipulation, locomotion, and maze-solving. OGBench standardizes data formats, evaluation protocols,
        and metrics, making it easier to compare algorithms fairly.
      </p>

      <p>
        For visual tasks, OGBench adopts an IMPALA-style convolutional encoder as the default backbone. This design
        choice isolates algorithmic differences—such as value learning, goal relabeling strategies, and sequence
        modeling—from representational differences. As a result, the majority of reported OGBench results focus on
        comparing offline RL algorithms under a fixed encoder, implicitly assuming that the representation is “good
        enough” and that further improvements should come from better credit assignment or planning.
      </p>

      <p>
        Our work complements this perspective by flipping the emphasis: we fix the algorithm (GCBC) and vary the visual
        encoder. OGBench then serves as a controlled testbed to assess how encoder choice impacts offline goal-conditioned
        control, holding everything else constant.
      </p>

      <h3>Visual Encoders for Control</h3>
      <p>
        A large body of work in visuomotor learning studies how to encode images into features suitable for control.
        Early RL systems relied on relatively shallow CNNs trained from scratch on individual tasks (e.g., Atari,
        DMControl), effectively letting the RL objective discover relevant features. The success of early convolutional
        architectures on large-scale image classification, such as AlexNet [1], further motivated the use of CNN
        backbones in control pipelines. More recent approaches incorporate
        pretraining, either via supervised learning (e.g., ImageNet-pretrained ResNets) or self-supervision (e.g.,
        contrastive learning, masked autoencoding).
      </p>

      <p>
        Convolutional architectures such as IMPALA and ResNet provide strong spatial inductive biases and have proven
        robust in RL, especially when normalized carefully. Vision Transformers (ViT) replace convolutions with
        patch-based self-attention and achieve strong performance in high-level vision benchmarks but are known to be
        data-hungry and less stable when trained from scratch on small datasets. Self-supervised models like DINOv3 [2]
        produce representations that are highly effective for classification and segmentation, but their invariances may
        obscure the fine-grained geometry needed for precise control.
      </p>

      <p>
        Prior work has evaluated individual encoder choices in specific robotics settings, but systematic comparisons of
        encoder families under a standardized offline GCRL benchmark are still rare. Our study aims to fill this gap.
      </p>

      <h2>Methods</h2>
      <p>
        We evaluate the impact of visual encoder architectures on offline goal-conditioned behavioral cloning using two
        visual tasks from OGBench. In all experiments, the learning algorithm, dataset, goal relabeling scheme, and
        policy head are held fixed; only the visual encoder is changed. This section gives a high-level overview of our
        GCBC formulation, encoder variants, and experimental setup. Full architectural and hyperparameter details are
        provided in the Appendix.
      </p>

      <h3>3.1 Goal-Conditioned Behavioral Cloning</h3>
      <p>
        The GCBC agent is written by OGBench. It treats each environment as an MDP \( (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma) \), where
        states \(s \in \mathcal{S}\) are RGB images and actions \(a \in \mathcal{A}\) are continuous control inputs. The
        offline dataset \( \mathcal{D} = \{ (s_t, a_t, s_{t+1}) \}_{t=1}^N \) consists of transitions collected by a
        behavior policy. To train a goal-conditioned policy, hindsight relabeling is performed: for a given time step \(t\)
        in a trajectory, a future state \(g\) is sampled from the same trajectory and treat it as the goal that the
        policy is trying to reach.
      </p>

      <p>
        Let \( \phi \) denote the visual encoder, mapping images to latent vectors. GCBC minimizes the negative
        log-likelihood of observed actions under the goal-conditioned policy:
      </p>

      <div style="text-align:center; margin: 1em 0;">
        \[
          \mathcal{L}_{\text{GCBC}}(\theta)
          = \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}, \, g \sim p(\cdot \mid s_{t:T})}
          \left[ - \log \pi_\theta(a_t \mid \phi(s_t), \phi(g)) \right].
        \]
      </div>

      <p>
        This objective is implemented as a standard supervised learning loss over minibatches drawn from the offline
        dataset, with goal relabeling applied on the fly.
      </p>

      <h3>3.2 Visual Encoders (High-Level)</h3>
      <p>
        All environments provide RGB observations of shape \(H \times W \times 3\). To capture short-term temporal
        information, we stack \(k = 3\) consecutive frames along the channel dimension, resulting in an input tensor of
        shape \(H \times W \times 9\). The visual encoder \( \phi \) maps this tensor to a latent embedding
        \( z \in \mathbb{R}^d \), which serves as input to the policy.
      </p>

      <p>
        We consider four encoders (View Appendix for implementation details):
      </p>
      <ul>
        <li><strong>IMPALA:</strong> The default OGBench convolutional encoder. It is relatively shallow but has strong
          spatial inductive biases and is well-tested in RL benchmarks.</li>
        <li><strong>ResNet50 (from scratch):</strong> A deeper convolutional network adapted for RL stability by
          replacing BatchNorm with GroupNorm. This increases capacity while preserving locality.</li>
        <li><strong>ViT-Small (from scratch):</strong> A compact Vision Transformer with patch-based self-attention,
          trained end-to-end on the offline trajectories. This tests the architectural inductive bias of transformers
          under limited data.</li>
        <li><strong>DINOv3-S/16 (frozen) [2]:</strong> A self-supervised ViT encoder used as a fixed feature extractor. Only
          the policy head is trained, probing the alignment between pretrained features and control objectives.</li>
      </ul>

      <p>
        In the main text we focus on the conceptual differences between these encoders—capacity, inductive bias, and
        pretraining—rather than implementation. Detailed layer configurations and embedding dimensions are
        given in Appendix A.
      </p>

      <h3>3.3 Policy Network and Action Distribution</h3>
      <p>
        For each transition, we encode the current image stack \(s\) and the goal image stack \(g\) using the chosen
        encoder \( \phi \), and concatenate their embeddings:
      </p>

      <div style="text-align:center; margin: 1em 0;">
        \[
          z_{\text{joint}} = [\phi(s); \phi(g)].
        \]
      </div>

      <p>
        This joint representation is passed through a 3-layer MLP with hidden size 512 and GELU activations to produce
        the parameters of a Gaussian policy. To enforce action bounds, we apply a component-wise \(\tanh\) to sampled
        actions:
      </p>

      <div style="text-align:center; margin: 1em 0;">
        \[
          \pi_\theta(a \mid s, g)
          = \tanh\!\left(\mu_\psi(z_{\text{joint}}) + \sigma_\psi(z_{\text{joint}})\cdot \epsilon\right), \quad
          \epsilon \sim \mathcal{N}(0, I).
        \]
      </div>

      <p>
        During training, stochastic sampling encourages the policy to explore the action space in imitation of the
        behavior policy. During evaluation, we use the mean action (temperature 0) for deterministic performance.
      </p>

      <h3>3.4 Experimental Setup</h3>

      <h4>3.4.1 Tasks</h4>
      <p>
        We evaluate encoders on two visual OGBench tasks that emphasize different aspects of perception and control:
      </p>
      <ul>
        <li>
          <strong>Visual Cube (Manipulation):</strong>
          In <code>visual-cube-single-play-v0</code>, a robotic hand must manipulate a cube so that its pose matches a
          target pose specified by a goal image. This is a high-precision geometric task where small changes in cube
          orientation or position can determine success. The encoder must preserve detailed local geometry and be robust
          to occlusions caused by the robot’s own fingers and palm.
        </li>
        <li>
          <strong>AntMaze-Medium (Navigation):</strong>
          In <code>antmaze-large-navigate-v0</code>, a quadrupedal ant robot must navigate a maze to reach a distant
          goal location. This requires both low-level locomotion control and high-level spatial reasoning: the encoder
          must represent the agent’s position relative to walls, corridors, and landmarks to support path planning.
        </li>
      </ul>

      <h4>3.4.2 Training Protocol</h4>
      <p>
        All models are trained for 500,000 gradient steps using the Adam optimizer with learning rate
        \( 3 \times 10^{-4} \). We use a batch size of 256 transitions per update. 
      </p>

      <p>
        Training is implemented in JAX/Flax with <code>jax.jit</code> compilation for efficient execution on a single
        GPU. A single random seed is used across encoder variants to facilitate fair comparison. Full optimizer, batch,
        and hardware details are reported in Appendix B.
      </p>

      <h4>3.4.3 Evaluation</h4>
      <p>
        After training, each model is evaluated over 50 episodes per task. At evaluation time OGBench uses deterministic
        actions by choosing the mean of the Gaussian policy. We report:
      </p>
      <ul>
        <li><strong>Success Rate:</strong> the fraction of episodes that terminate in a goal state within the prescribed
          tolerance.</li>
        <li><strong>Episode Return:</strong> the cumulative sparse reward, which is positive when goals are reached.</li>
      </ul>

      <p>
        To better understand training dynamics, we also track GCBC training loss and a held-out validation loss over
        training, but we defer these curves to Appendix C to keep the main Results section focused on task performance.
      </p>

      <h2>Results</h2>
      <table>
        <thead>
          <tr>
            <th style="text-align:left;">Encoder</th>
            <th style="text-align:center;">AntMaze-Medium</th>
            <th style="text-align:center;">Single-Cube</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>IMPALA</td>
            <td style="text-align:center; font-weight:600;">0.74</td>
            <td style="text-align:center;">0.02</td>
          </tr>
          <tr>
            <td>ResNet50</td>
            <td style="text-align:center;">0.108</td>
            <td style="text-align:center; font-weight:600;">0.132</td>
          </tr>
          <tr>
            <td>DINOv3-S/16</td>
            <td style="text-align:center;">0.076</td>
            <td style="text-align:center;">0.00</td>
          </tr>
          <tr>
            <td>ViT-Small</td>
            <td style="text-align:center;">0.03</td>
            <td style="text-align:center;">0.00</td>
          </tr>
        </tbody>
      </table>

      <p style="text-align:center; font-family:'JetBrains Mono', monospace; font-size:0.85rem; color:#666;">
        Table 1: Success rates of different visual encoders under GCBC on two OGBench visual tasks.
      </p>

      <h3>4.1 AntMaze-Medium</h3>
      <p>
        On the AntMaze-Medium navigation task, the baseline IMPALA encoder achieves the highest success rate (0.74) by a
        large margin. This is noteworthy because IMPALA is the shallowest encoder we consider. Its strong performance
        suggests that, for this task, a relatively low-capacity convolutional network with strong locality bias and
        stable optimization is sufficient to extract the global geometry needed for navigation.
      </p>

      <p>
        ResNet50 and DINOv3-S/16 [2] achieve moderate but clearly inferior performance (0.108 and 0.076, respectively),
        while the ViT-Small performs the worst (0.03). When we inspect training and validation losses (Appendix C), we find
        that ResNet50 and DINOv3 converge more slowly and exhibit higher variance across epochs, consistent with more
        unstable optimization. The ViT-Small shows clear signs of underfitting or representation collapse: loss decreases
        initially but plateaus at a high value, indicating that the model fails to fully exploit the available data.
      </p>

      <p>
        One plausible explanation is that AntMaze requires a mixture of local and global reasoning: the agent needs to
        understand maze topology and wall locations but does not require extremely fine-grained texture or pose
        estimation. IMPALA’s inductive bias and modest depth may strike the right balance here. In contrast, deeper
        networks and transformers might be over-parameterized for the dataset size, making them harder to train
        effectively in an offline regime without pretraining.
      </p>

      <h3>4.2 Visual Cube Manipulation</h3>
      <p>
        On the Visual Cube manipulation task, the ranking of encoders changes dramatically. Here, ResNet50 achieves the
        highest success rate (0.132), substantially outperforming IMPALA (0.02), the baseline, and the transformer-based encoders
        (both 0.00). This supports the hypothesis that deeper convolutional models are better suited to tasks that
        require precise geometric reasoning.
      </p>

      <p>
        Visual Cube demands accurate perception of cube orientation, edge alignment, and depth cues under frequent
        occlusions from the robot hand. Our qualitative analysis suggests that ResNet50’s deeper feature hierarchy
        enables it to build more robust representations of cube pose than IMPALA, which appears to capture only coarse
        shape and position. The training and validation curves (Appendix C) show that ResNet50 converges to a lower
        imitation loss on this task than other encoders, consistent with its superior performance.
      </p>

      <p>
        Our ViT-Small and DINOv3-S/16 [2] both fail to solve the task, achieving zero success rate. This aligns with the intuition
        that compressing all spatial information into a single CLS token without explicit spatial inductive biases
        makes it difficult to preserve fine-grained pose information. Even though DINOv3 [2] is pretrained, its invariances
        to cropping, color jitter, and viewpoint are likely misaligned with the demands of low-level manipulation, where
        small visual differences correspond to large changes in the underlying state.
      </p>

      <h3>Insights</h3>
      <p>
        Comparing across both tasks, we see that:
      </p>
      <ul>
        <li>IMPALA excels at AntMaze but struggles on Visual Cube.</li>
        <li>ResNet50 shows the opposite pattern: weaker on AntMaze but clearly superior on Visual Cube.</li>
        <li>Transformer-based encoders (ViT-Small, DINOv3-S/16) perform poorly in both settings under our training
          protocol.</li>
      </ul>
      <p>
        These observations support a nuanced view: encoder performance in offline GCRL depends not only
        on capacity or pretraining quality, but also on how inductive biases and invariances interact with the specific
        structure of the task.
      </p>

      <h2>Discussion and Future Work</h2>
      <p>
        Our experiments show that visual encoders play a decisive role in offline goal-conditioned reinforcement learning
        and behavioral cloning. There is no single “best” encoder across tasks: shallow CNNs like IMPALA can outperform
        deeper models on navigation tasks, while deeper ResNets can be crucial for precise geometric manipulation. At the
        same time, transformer-based encoders—both from scratch and pretrained—do not automatically translate their
        success in high-level vision tasks into strong control performance under the constraints of OGBench.
      </p>

      <p>
        These results suggest several directions for future work. First, alternative pooling strategies for transformer
        features (e.g., spatial softmax, coordinate heads, or hybrid CNN–ViT architectures) might help preserve
        geometry without sacrificing the benefits of global attention. Second, lightweight task-specific adaptation of
        pretrained encoders, such as adapter layers or partial fine-tuning, could better align self-supervised features
        with control objectives while retaining the benefits of large-scale pretraining.
      </p>

      <p>
        A broader takeaway is methodological: encoder choice should be treated as a first-class design axis in control
        research, particularly in offline settings where the data distribution is fixed and representation learning is
        constrained. Benchmarks like OGBench are invaluable for algorithm comparison, but they also provide an
        opportunity to systematically investigate how representation learning affects control. We hope that this work
        encourages further studies at the intersection of visual encoding and offline goal-conditioned RL, and that
        future benchmarks will incorporate representation-focused baselines more explicitly.
      </p>
    </article>

    
    <article>
      <h2>Appendix</h2>

      <h3>A. Encoder Architecture Details</h3>
      <p>
        This section describes the encoder architectures used in our experiments in greater detail. All encoders operate
        on stacked RGB frames of shape \(H \times W \times 3k\) with \(k = 3\). Unless otherwise specified, we use
        \(H = W = 64\).
      </p>

      <h4>A.1 IMPALA</h4>
      <p>
        The IMPALA encoder follows the standard architecture used in many RL benchmarks:
      </p>
      <ul>
        <li>Three convolutional blocks with ReLU activations.</li>
        <li>Each block consists of a \(3 \times 3\) convolution with stride 2 followed by a \(3 \times 3\) convolution
          with stride 1.</li>
        <li>Channel sizes increase from 16 → 32 → 32 across blocks.</li>
        <li>The final feature map is flattened and passed through a linear layer to obtain a latent embedding
          \( z \in \mathbb{R}^{d} \), where \(d\) matches the policy input dimension used by OGBench.</li>
      </ul>

      <h4>A.2 ResNet50 (RL Variant)</h4>
      <p>
        Our ResNet50 backbone is adapted from the standard ImageNet configuration but modified for RL stability:
      </p>
      <ul>
        <li>All <strong>Batch Normalization</strong> layers are replaced by <strong>Group Normalization</strong> (GN)
          with 32 groups. This removes dependence on batch-level statistics and mitigates instability with correlated
          trajectory samples.</li>
        <li>The network uses an initial \(7 \times 7\) stride-2 convolution followed by a \(3 \times 3\) max-pooling
          layer, and four residual stages with bottleneck blocks and channel sizes 256, 512, 1024, and 2048.</li>
        <li>We apply global average pooling (GAP) over the final feature map to obtain a vector \(h \in \mathbb{R}^{2048}\)
          per frame.</li>
        <li>For frame stacks, the same ResNet is applied to each frame independently; the resulting vectors are
          concatenated to form a representation in \(\mathbb{R}^{2048 \times 3}\), which is then projected through a
          linear layer to the policy embedding dimension.</li>
      </ul>

      <h4>A.3 ViT-Small (from scratch)</h4>
      <p>
        The ViT-Small encoder is a compact Vision Transformer:
      </p>
      <ul>
        <li>Each image frame is split into non-overlapping \(16 \times 16\) patches.</li>
        <li>Patches are flattened and linearly projected to an embedding dimension \(D = 384\).</li>
        <li>A learnable [CLS] token is prepended to the sequence, and learnable positional embeddings are added to all tokens.</li>
        <li>The sequence is passed through 12 transformer encoder blocks, each with 6 attention heads, MLP dimension
          1536, LayerNorm, and residual connections.</li>
        <li>After the final block, we discard patch tokens and use the [CLS] token as the frame representation.</li>
        <li>Frames in the stack are processed in parallel (by reshaping the batch), and their [CLS] tokens are
          concatenated and linearly projected to the policy embedding dimension.</li>
      </ul>

      <h4>A.4 DINOv3-S/16 (Frozen)</h4>
      <p>
        For DINOv3-S/16, we use an off-the-shelf pretrained checkpoint provided by the original authors:
      </p>
      <ul>
        <li>Architecture is essentially identical to ViT-Small (patch size 16, embedding dimension 384, 12 blocks, 6
          heads).</li>
        <li>We use the [CLS] token as the global representation for each frame.</li>
        <li>Encoder weights are frozen: we stop gradients at the encoder output and only train the downstream policy
          head.</li>
      </ul>

      <h3>B. Training Procedure and Hyperparameters</h3>
      <p>
        This section lists the main hyperparameters used in our experiments.
      </p>
      <ul>
        <li><strong>Optimizer:</strong> Adam</li>
        <li><strong>Learning rate:</strong> \(3 \times 10^{-4}\)</li>
        <li><strong>Batch size:</strong> 256 transitions</li>
        <li><strong>Number of gradient steps:</strong> 500,000</li>
        <li><strong>Discount factor \(\gamma\):</strong> 0.99 (used only in environment reward computation, not in the
          supervised GCBC loss)</li>
        <li><strong>Frame stack:</strong> 3 consecutive frames</li>
        <li><strong>Data augmentation:</strong> random crop with probability 0.5 (4-pixel padding and crop back to
          \(64 \times 64\))</li>
        <li><strong>Activation function:</strong> GELU in the policy MLP, ReLU in CNNs, GELU in ViTs</li>
        <li><strong>Hardware:</strong> single NVIDIA GPU (e.g., RTX 3090) with JAX/Flax and XLA acceleration</li>
      </ul>

      <p>
        All experiments use the same random seed for fair comparison across encoder variants. We did not perform
        extensive hyperparameter tuning for individual encoders; rather, we chose a single GCBC configuration that was
        stable across all models.
      </p>

      <h3>C. Training and Validation Loss Curves</h3>
      <p>
        To better understand optimization dynamics, we track both training loss (on mini-batches sampled from the
        offline dataset) and validation loss (on a held-out subset of trajectories) over the course of training.
      </p>

      <p>
        Qualitatively, we observe the following trends:
      </p>
      <ul>
        <li><strong>IMPALA:</strong> training and validation losses decrease rapidly and plateau at relatively low
          values on both tasks, indicating that the model fits the data well without severe overfitting.</li>
        <li><strong>ResNet50:</strong> training loss decreases steadily, but validation loss flattens at a higher value
          on AntMaze than on Visual Cube, consistent with its weaker performance on navigation and stronger performance
          on manipulation.</li>
        <li><strong>ViT-Small:</strong> both training and validation losses decrease initially but plateau early at
          relatively high values, suggesting underfitting or difficulty in optimization.</li>
        <li><strong>DINOv3-S/16:</strong> validation loss decreases slightly but remains worse than ResNet50 and IMPALA
          on both tasks, consistent with the mismatch between self-supervised invariances and control objectives.</li>
      </ul>

      <p>
        In a full paper version, we would include plots of these curves (e.g., Figure C.1–C.4) for each encoder-task
        pair. For brevity, we omit the rendered figures here but follow standard practice in logging and monitoring.
      </p>
      
    </article>

    <h2>References</h2>
    <ol style="line-height:1.6">

      <li>
        A. Krizhevsky, I. Sutskever, and G. E. Hinton.
        <strong>ImageNet classification with deep convolutional neural networks.</strong>
        In <em>Proceedings of the 26th International Conference on Neural Information Processing Systems</em>, 
        pages 1097–1105, 2012.
      </li>

      <li>
        O. Siméoni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov,
        M. Szafraniec, S. Yi, M. Ramamonjisoa, F. Massa, D. Haziza, L. Wehrstedt,
        J. Wang, T. Darcet, T. Moutakanni, L. Sentana, C. Roberts, A. Vedaldi, 
        J. Tolan, J. Brandt, C. Couprie, J. Mairal, H. Jégou, P. Labatut, and P. Bojanowski.
        <strong>DINOv3.</strong>
        <em>arXiv preprint arXiv:2508.10104</em>, 2025.
      </li>

      <li>
        S. Park, K. Frans, B. Eysenbach, and S. Levine.
        <strong>OGBench: Benchmarking offline goal-conditioned RL.</strong>
        <em>arXiv preprint arXiv:2410.20092</em>, 2025.
      </li>

      <li>
        N. Rudin, D. Hoeller, P. Reist, and M. Hutter.
        <strong>Learning to walk in minutes using massively parallel deep reinforcement learning.</strong>
        <em>arXiv preprint arXiv:2109.11978</em>, 2021.
      </li>

      <li>
        D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen,
        E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine.
        <strong>QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation.</strong>
        <em>arXiv preprint arXiv:1806.10293</em>, 2018.
      </li>

      <li>
        OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. Leoni Aleman,
        D. Almeida, J. Altenschmidt, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji,
        V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine,
        G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd,
        and many others.
        <strong>GPT-4 technical report.</strong>
        <em>arXiv preprint arXiv:2303.08774</em>, 2024.
      </li>

    </ol>
  </div>
</body>
</html>

