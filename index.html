<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Evaluation of Visual Encoders for Offline Goal-Conditioned Reinforcement Learning and Behavioral Cloning in OGBench</title>

  <!-- Inter + JetBrains Mono -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=JetBrains+Mono:wght@300;400;500&display=swap" rel="stylesheet">

  <style>
    :root {
      --bg: #fafafa;
      --text: #111;
      --muted: #666;
      --code-bg: #f3f3f3;
      --max-width: 760px;
    }

    body {
      margin: 0;
      padding: 0;
      background: var(--bg);
      color: var(--text);
      font-family: "Inter", sans-serif;
      line-height: 1.65;
      font-size: 16px;
      -webkit-font-smoothing: antialiased;
    }

    .wrapper {
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 2rem 1.5rem 6rem;
    }

    header {
      margin-bottom: 3.5rem;
    }

    header h1 {
      font-family: "JetBrains Mono", monospace;
      font-size: 2rem;
      font-weight: 500;
      margin: 0 0 1rem;
    }

    header p {
      margin: 0;
      color: var(--muted);
      font-family: "JetBrains Mono", monospace;
      font-size: 0.9rem;
    }

    .header-links {
      margin-top: 1rem;
      display: flex;
      gap: 1.5rem;
      flex-wrap: wrap;
    }

    .header-links a {
      color: var(--text);
      text-decoration: none;
      font-family: "JetBrains Mono", monospace;
      font-size: 0.9rem;
      padding: 0.4rem 0.8rem;
      border: 1px solid var(--muted);
      border-radius: 4px;
      transition: all 0.2s ease;
    }

    .header-links a:hover {
      background: var(--muted);
      color: var(--bg);
      border-color: var(--text);
    }

    @media (prefers-color-scheme: dark) {
      .header-links a {
        border-color: var(--muted);
      }
      .header-links a:hover {
        background: var(--muted);
        color: var(--bg);
      }
    }

    article {
      margin-bottom: 4rem;
    }

    article h2 {
      font-family: "JetBrains Mono", monospace;
      margin-top: 2.5rem;
      margin-bottom: 1rem;
      font-size: 1.5rem;
      font-weight: 500;
    }

    article h3 {
      font-family: "JetBrains Mono", monospace;
      margin-top: 2rem;
      margin-bottom: 0.75rem;
      font-size: 1.1rem;
      font-weight: 500;
    }

    article h4 {
      font-family: "JetBrains Mono", monospace;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
      font-size: 1rem;
      font-weight: 500;
    }

    article p {
      margin-bottom: 1.2rem;
    }

    article ul {
      margin: 0 0 1.2rem 1.2rem;
      padding-left: 1rem;
    }

    code {
      font-family: "JetBrains Mono", monospace;
      background: var(--code-bg);
      padding: 2px 5px;
      border-radius: 4px;
      font-size: 0.95rem;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      font-family: "JetBrains Mono", monospace;
      font-size: 0.95rem;
    }

    th, td {
      padding: 8px;
    }

    thead tr {
      border-bottom: 2px solid #ccc;
    }

    tbody tr {
      border-bottom: 1px solid #eee;
    }

    footer {
      margin-top: 4rem;
      padding-top: 2rem;
      text-align: center;
      font-family: "JetBrains Mono", monospace;
      font-size: 0.8rem;
      color: var(--muted);
    }

    figure {
      margin: 2rem 0;
      text-align: center;
    }

    figure img {
      max-width: 100%;
      height: auto;
      border-radius: 4px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }

    figure video {
      max-width: 100%;
      height: auto;
      border-radius: 4px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      background: #000;
    }

    figure figcaption {
      margin-top: 0.75rem;
      font-family: "JetBrains Mono", monospace;
      font-size: 0.85rem;
      color: var(--muted);
      line-height: 1.5;
    }

    .figure-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1.5rem;
      margin: 2rem 0;
    }

    @media (max-width: 600px) {
      .figure-grid {
        grid-template-columns: 1fr;
      }
    }

    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0f0f0f;
        --text: #eee;
        --muted: #aaa;
        --code-bg: #1b1b1b;
      }
    }
  </style>

  <!-- MathJax for LaTeX rendering -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(", "\\)"]],
        displayMath: [["\\[", "\\]"]]
      }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
  <div class="wrapper">
    <header>
      <h1>Evaluation of Visual Encoders for Offline Goal-Conditioned Reinforcement Learning and Behavioral Cloning in OGBench</h1>
      <p>Published: Dec. 9, 2025</p>
      <p>Authors: Yash Prabhu<sup>*</sup> (<a href="mailto:yprabhu@mit.edu">yprabhu@mit.edu</a>), Richard Lian<sup>*</sup> (<a href="mailto:rlian@mit.edu">rlian@mit.edu</a>)</p>
      <p><sup>*</sup> denotes equal contribution</p>
      <div class="header-links">
        <a href="https://wandb.ai/yprabhu-massachusetts-institute-of-technology/OGBench" target="_blank" rel="noopener noreferrer">üìä Weights & Biases</a>
        <a href="https://github.com/yprabhu23/ogbench" target="_blank" rel="noopener noreferrer">üíª GitHub Repository</a>
      </div>
    </header>

    <article>
      <h2>Motivations</h2>
      <p>
        Over the past decade, the landscape of artificial intelligence has changed almost beyond recognition.
        The modern era of deep learning was catalyzed in 2012 by AlexNet <a href="#ref1">[1]</a>, a convolutional
        network that dramatically outperformed traditional pipelines on the ImageNet benchmark. AlexNet did more than
        win a competition: it provided a concrete demonstration that large, deep neural networks could be trained at
        scale and deployed on real-world perception problems. In the years that followed, deep learning rapidly
        expanded through architectures such as LSTMs and RNNs for sequence modeling, Transformers for attention-based
        reasoning, and generative models such as diffusion and flow-based models for high-fidelity synthesis.
      </p>

      <p>
        These architectural innovations, combined with large datasets and powerful hardware, culminated in today‚Äôs
        foundation models. Large language models like ChatGPT, Gemini, and Claude exhibit strikingly general capabilities:
        they can summarize, translate, write code, reason about instructions, and sometimes even plan at a high level.
        It is tempting to view these systems as the first step toward universal ‚Äúworld models‚Äù that can flexibly adapt
        to new tasks given only minimal prompting.
      </p>

      <p>
        However, the success of foundation models has been concentrated in digital and symbolic domains. A parallel
        effort in the research community and industry aims to bring comparable generality to <em>embodied</em> systems‚Äî
        robots and agents that perceive and act in the physical world. Startups and labs such as Physical Intelligence,
        SKILD, Figure, and others share a common dream: leverage large-scale representation learning and reinforcement
        learning (RL) to build general-purpose control systems that can manipulate objects, navigate complex
        environments, and adapt to new tasks with relatively little additional training.
      </p>

      <p>
        Realizing this vision requires bridging a substantial gap between representation learning and control.
        Perception-only world models, such as those learned by self-supervised visual encoders, are not sufficient on
        their own: an embodied agent must transform high-dimensional sensory input (e.g., images) into temporally
        coherent, goal-directed actions under uncertainty. Reinforcement learning provides a natural mathematical
        framework for this transformation. In RL, an agent interacts with an environment, receives observations, selects
        actions, and is guided by scalar rewards that encode task performance. RL has produced impressive results in
        locomotion, manipulation, and navigation, especially in simulation. Yet classical RL typically assumes
        low-dimensional state representations, carefully engineered rewards, and extensive online interaction‚Äîassumptions
        that are difficult to satisfy in real-world robotics.
      </p>

      <p>
        Goal-Conditioned Reinforcement Learning (GCRL) relaxes some of these constraints by treating the desired outcome
        of an episode as an explicit input to the policy. Rather than learning a single task-specific behavior, the agent
        aims to reach arbitrary goal states \(g\), making it possible to train a <em>family</em> of behaviors
        parameterized by the goal. When combined with offline learning and behavioral cloning, GCRL can be trained
        entirely from static datasets of trajectories, without requiring online access to the environment. This is
        particularly appealing for robotics, where data collection is expensive and risky.
      </p>

      <p>
        In vision-based GCRL, the agent‚Äôs observations and goals are raw images. The quality of behavior is therefore
        tightly coupled to the choice of visual encoder that maps pixels to latent representations. Historically, many
        RL pipelines have relied on relatively shallow convolutional networks such as IMPALA, which are easy to train
        from scratch and provide strong locality biases. At the same time, modern vision research has produced a wide
        range of powerful encoders, including deep ResNets, Vision Transformers (ViT), and self-supervised models like
        DINOv3 <a href="#ref2">[2]</a>. These models achieve impressive performance on classification, segmentation, and retrieval, but it
        remains unclear whether‚Äîand how‚Äîtheir representations transfer to offline goal-conditioned control.
      </p>

      <p>
        This motivates the central question of our study:
        <br><br>
        <strong>How does the choice of visual encoder affect the performance of offline goal-conditioned reinforcement
        learning and behavioral cloning on pixel-based tasks?</strong>
        <br><br>
        To answer this question, we turn to OGBench <a href="#ref3">[3]</a>, a benchmark that standardizes offline GCRL evaluation across dozens
        of environments. OGBench uses an IMPALA-style encoder by default, effectively fixing the representation and
        varying only the algorithm. In contrast, we fix the algorithm to goal-conditioned behavioral cloning (GCBC) and
        systematically vary the visual encoder.
      </p>

      <p>
        Concretely, we compare four encoders‚ÄîIMPALA, an RL-adapted ResNet50, a Vision Transformer (ViT-Small) trained
        from scratch, and a frozen DINOv3-S/16 backbone‚Äîon two visual OGBench tasks: a cube manipulation environment
        and an AntMaze navigation task. Our experiments show that these encoders exhibit complementary strengths and
        weaknesses, and that blindly importing state-of-the-art vision models into offline RL pipelines often fails to
        yield the expected gains. The rest of this article formalizes the goal-conditioned setting, surveys related work,
        describes our methods at a high level, presents empirical results, and concludes with a discussion of open
        questions. Detailed architecture specifications, training hyperparameters, loss curves, representation analyses,
        and environment settings are provided in the Appendix.
      </p>

      <h2>Related Works</h2>

      <h3>Goal-Conditioned Reinforcement Learning</h3>
      <p>
        Goal-conditioned reinforcement learning (GCRL) extends the standard RL formulation by conditioning the policy
        and value functions on a goal state. We consider a controlled Markov process defined by state space
        \( \mathcal{S} \), action space \( \mathcal{A} \), initial state distribution \( \mu(s) \), and transition
        dynamics \( p(s' \mid s, a) \). In contrast to reward-centric formulations, we assume the environment dynamics
        are given but do not rely on an externally specified reward function during training.
      </p>

      <p>
        In the offline setting, the agent observes a static dataset of trajectories:
      </p>

      <p style="text-align:center;">
        \( \mathcal{D} = \left\{ (s_0^{(n)}, a_0^{(n)}, s_1^{(n)}, \ldots, s_{T_n}^{(n)}) \right\}_{n=1}^{N}, \)
      </p>

      <p>
        collected by one or more behavior policies. The agent cannot gather additional experience. The objective is to
        learn a goal-conditioned policy \( \pi(a \mid s, g) \) that can reliably reach a target state \( g \in
        \mathcal{S} \) from diverse starting states. A common formalization is to maximize the expected discounted
        probability of hitting the goal:
      </p>

      <div style="text-align:center; margin: 1em 0;">
        \[
          \mathbb{E}_{\tau \sim p(\tau \mid g)}
          \left[ \sum_{t=0}^{T} \gamma^{t} \mathbf{1}[s_t = g] \right],
        \]
      </div>

      <p>
        where \( \gamma \in (0,1) \) is a discount factor and \( p(\tau \mid g) \) is the trajectory distribution
        induced by conditioning on goal \(g\).
      </p>

      <p>
        In practice, directly optimizing this objective is difficult, and many offline GCRL methods make use of goal
        relabeling or hindsight techniques. Goal-conditioned behavioral cloning (GCBC), which is the agent of choice in this work,
        follows a particularly simple strategy: treat future states in a trajectory as goals for earlier states, and
        train a supervised policy to imitate the observed actions given these goals. Despite its simplicity, GCBC has
        proven surprisingly effective in offline settings, especially when combined with large and diverse datasets.
      </p>

      <h3>OGBench</h3>
      <p>
        OGBench <a href="#ref3">[3]</a> is a benchmark suite designed to evaluate offline goal-conditioned RL methods across a diverse set of
        tasks. It includes both low-dimensional state-based environments and higher-dimensional visual tasks, covering
        navigation, manipulation, locomotion, and maze-solving. OGBench standardizes data formats, evaluation protocols,
        and metrics, making it easier to compare algorithms fairly.
      </p>

      <p>
        For visual tasks, OGBench adopts an IMPALA-style convolutional encoder as the default backbone. This design
        choice isolates algorithmic differences‚Äîsuch as value learning, goal relabeling strategies, and sequence
        modeling‚Äîfrom representational differences. As a result, the majority of reported OGBench results focus on
        comparing offline RL algorithms under a fixed encoder, implicitly assuming that the representation is ‚Äúgood
        enough‚Äù and that further improvements should come from better credit assignment or planning.
      </p>

      <p>
        Our work complements this perspective by flipping the emphasis: we fix the algorithm (GCBC) and vary the visual
        encoder. OGBench then serves as a controlled testbed to assess how encoder choice impacts offline goal-conditioned
        control, holding everything else constant.
      </p>

      <h3>Visual Encoders for Control</h3>
      <p>
        A large body of work in visuomotor learning studies how to encode images into features suitable for control.
        Early RL systems relied on relatively shallow CNNs trained from scratch on individual tasks (e.g., Atari,
        DMControl), effectively letting the RL objective discover relevant features. The success of early convolutional
        architectures on large-scale image classification, such as AlexNet <a href="#ref1">[1]</a>, further motivated the use of CNN
        backbones in control pipelines. More recent approaches incorporate
        pretraining, either via supervised learning (e.g., ImageNet-pretrained ResNets) or self-supervision (e.g.,
        contrastive learning, masked autoencoding).
      </p>

      <p>
        Convolutional architectures such as IMPALA and ResNet provide strong spatial inductive biases and have proven
        robust in RL, especially when normalized carefully. Vision Transformers (ViT) replace convolutions with
        patch-based self-attention and achieve strong performance in high-level vision benchmarks but are known to be
        data-hungry and less stable when trained from scratch on small datasets. Self-supervised models like DINOv3 <a href="#ref2">[2]</a>
        produce representations that are highly effective for classification and segmentation, but their invariances may
        obscure the fine-grained geometry needed for precise control.
      </p>

      <p>
        Prior work has evaluated individual encoder choices in specific robotics settings, but systematic comparisons of
        encoder families under a standardized offline GCRL benchmark are still rare. Our study aims to fill this gap.
      </p>

      <h2>Methods</h2>
      <p>
        We evaluate the impact of visual encoder architectures on offline goal-conditioned behavioral cloning using two
        visual tasks from OGBench. In all experiments, the learning algorithm, dataset, goal relabeling scheme, and
        policy head are held fixed; only the visual encoder is changed. This section gives a high-level overview of our
        GCBC formulation, encoder variants, and experimental setup. Full architectural and hyperparameter details are
        provided in the Appendix.
      </p>

      <h3>3.1 Goal-Conditioned Behavioral Cloning</h3>
      <p>
        The GCBC agent is provided by OGBench. It treats each environment as an MDP \( (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma) \), where
        states \(s \in \mathcal{S}\) are RGB images and actions \(a \in \mathcal{A}\) are continuous control inputs. The
        offline dataset \( \mathcal{D} = \{ (s_t, a_t, s_{t+1}) \}_{t=1}^N \) consists of transitions collected by a
        behavior policy. To train a goal-conditioned policy, hindsight relabeling is performed: for a given time step \(t\)
        in a trajectory, a future state \(g\) is sampled from the same trajectory and treat it as the goal that the
        policy is trying to reach.
      </p>

      <p>
        Let \( \phi \) denote the visual encoder, mapping images to latent vectors. GCBC minimizes the negative
        log-likelihood of observed actions under the goal-conditioned policy:
      </p>

      <div style="text-align:center; margin: 1em 0;">
        \[
          \mathcal{L}_{\text{GCBC}}(\theta)
          = \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}, \, g \sim p(\cdot \mid s_{t:T})}
          \left[ - \log \pi_\theta(a_t \mid \phi(s_t), \phi(g)) \right].
        \]
      </div>

      <p>
        This objective is implemented as a standard supervised learning loss over minibatches drawn from the offline
        dataset, with goal relabeling applied on the fly.
      </p>

      <h3>3.2 Visual Encoders (High-Level)</h3>
      <p>
        All environments provide RGB observations of shape \(H \times W \times 3\). To capture short-term temporal
        information, we stack \(k = 3\) consecutive frames along the channel dimension, resulting in an input tensor of
        shape \(H \times W \times 9\). The visual encoder \( \phi \) maps this tensor to a latent embedding
        \( z \in \mathbb{R}^d \), which serves as input to the policy.
      </p>

      <p>
        We consider four encoders (View Appendix for implementation details):
      </p>
      <ul>
        <li><strong>IMPALA:</strong> The default OGBench convolutional encoder. It is relatively shallow but has strong
          spatial inductive biases and is well-tested in RL benchmarks.</li>
        <li><strong>ResNet50 (from scratch):</strong> A deeper convolutional network adapted for RL stability by
          replacing BatchNorm with GroupNorm. This increases capacity while preserving locality.</li>
        <li><strong>ViT-Small (from scratch):</strong> A compact Vision Transformer with patch-based self-attention,
          trained end-to-end on the offline trajectories. This tests the architectural inductive bias of transformers
          under limited data.</li>
        <li><strong>DINOv3-S/16 (frozen) <a href="#ref2">[2]</a>:</strong> A self-supervised ViT encoder used as a fixed feature extractor. Only
          the policy head is trained, probing the alignment between pretrained features and control objectives.</li>
      </ul>

      <p>
        In the main text we focus on the conceptual differences between these encoders‚Äîcapacity, inductive bias, and
        pretraining‚Äîrather than implementation. Detailed layer configurations and embedding dimensions are
        given in Appendix A.
      </p>

      <h3>3.3 Policy Network and Action Distribution</h3>
      <p>
        For each transition, we encode the current image stack \(s\) and the goal image stack \(g\) using the chosen
        encoder \( \phi \), and concatenate their embeddings:
      </p>

      <div style="text-align:center; margin: 1em 0;">
        \[
          z_{\text{joint}} = [\phi(s); \phi(g)].
        \]
      </div>

      <p>
        This joint representation is passed through a 3-layer MLP with hidden size 512 and GELU activations to produce
        the parameters of a Gaussian policy. To enforce action bounds, we apply a component-wise \(\tanh\) to sampled
        actions:
      </p>

      <div style="text-align:center; margin: 1em 0;">
        \[
          \pi_\theta(a \mid s, g)
          = \tanh\!\left(\mu_\psi(z_{\text{joint}}) + \sigma_\psi(z_{\text{joint}})\cdot \epsilon\right), \quad
          \epsilon \sim \mathcal{N}(0, I).
        \]
      </div>

      <p>
        During training, stochastic sampling encourages the policy to explore the action space in imitation of the
        behavior policy. During evaluation, we use the mean action (temperature 0) for deterministic performance.
      </p>

      <h3>3.4 Experimental Setup</h3>

      <h4>3.4.1 Tasks</h4>
      <p>
        We evaluate encoders on two visual OGBench tasks that emphasize different aspects of perception and control:
      </p>
      <ul>
        <li>
          <strong>Visual Cube (Manipulation):</strong>
          In <code>visual-cube-single-play-v0</code>, a robotic hand must manipulate a cube so that its pose matches a
          target pose specified by a goal image. This is a high-precision geometric task where small changes in cube
          orientation or position can determine success. The encoder must preserve detailed local geometry and be robust
          to occlusions caused by the robot‚Äôs own fingers and palm.
        </li>
        <li>
          <strong>AntMaze-Medium (Navigation):</strong>
          In <code>antmaze-large-navigate-v0</code>, a quadrupedal ant robot must navigate a maze to reach a distant
          goal location. This requires both low-level locomotion control and high-level spatial reasoning: the encoder
          must represent the agent‚Äôs position relative to walls, corridors, and landmarks to support path planning.
        </li>
      </ul>

      <h4>3.4.2 Training Protocol</h4>
      <p>
        All models are trained for 500,000 gradient steps using the Adam optimizer with learning rate
        \( 3 \times 10^{-4} \). We use a batch size of 256 transitions per update. 
      </p>

      <p>
        Training is implemented in JAX/Flax with <code>jax.jit</code> compilation for efficient execution on a single
        GPU. A single random seed is used across encoder variants to facilitate fair comparison. Full optimizer, batch,
        and hardware details are reported in Appendix B.
      </p>

      <h4>3.4.3 Evaluation</h4>
      <p>
        After training, each model is evaluated over 50 episodes per task. At evaluation time OGBench uses deterministic
        actions by choosing the mean of the Gaussian policy. We report:
      </p>
      <ul>
        <li><strong>Success Rate:</strong> the fraction of episodes that terminate in a goal state within the prescribed
          tolerance.</li>
        <li><strong>Episode Return:</strong> the cumulative sparse reward, which is positive when goals are reached.</li>
      </ul>

      <p>
        To better understand training dynamics, we also track GCBC training loss and a held-out validation loss over
        training, but we defer these curves to Appendix C to keep the main Results section focused on task performance.
      </p>

      <h2>Results</h2>
      <table>
        <thead>
          <tr>
            <th style="text-align:left;">Encoder</th>
            <th style="text-align:center;">AntMaze-Medium</th>
            <th style="text-align:center;">Single-Cube</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>IMPALA</td>
            <td style="text-align:center; font-weight:600;">0.74</td>
            <td style="text-align:center;">0.02</td>
          </tr>
          <tr>
            <td>ResNet50</td>
            <td style="text-align:center;">0.108</td>
            <td style="text-align:center; font-weight:600;">0.132</td>
          </tr>
          <tr>
            <td>DINOv3-S/16</td>
            <td style="text-align:center;">0.076</td>
            <td style="text-align:center;">0.00</td>
          </tr>
          <tr>
            <td>ViT-Small</td>
            <td style="text-align:center;">0.03</td>
            <td style="text-align:center;">0.00</td>
          </tr>
        </tbody>
      </table>

      <p style="text-align:center; font-family:'JetBrains Mono', monospace; font-size:0.85rem; color:#666;">
        Table 1: Success rates of different visual encoders under GCBC on two OGBench visual tasks.
      </p>

      <h3>4.1 AntMaze-Medium</h3>
      <p>
        On the AntMaze-Medium navigation task, the baseline IMPALA encoder achieves the highest success rate (0.74) by a
        large margin. This is noteworthy because IMPALA is the shallowest encoder we consider. Its strong performance
        suggests that, for this task, a relatively low-capacity convolutional network with strong locality bias and
        stable optimization is sufficient to extract the global geometry needed for navigation.
      </p>

      <p>
        ResNet50 and DINOv3-S/16 <a href="#ref2">[2]</a> achieve moderate but clearly inferior performance (0.108 and 0.076, respectively),
        while the ViT-Small performs the worst (0.03). When we inspect training and validation losses, we find
        that ResNet50 and DINOv3 converge more slowly and exhibit higher variance across epochs, consistent with more
        unstable optimization. The ViT-Small shows clear signs of underfitting or representation collapse: loss decreases
        initially but plateaus at a high value, indicating that the model fails to fully exploit the available data.
      </p>

      <div class="figure-grid">
        <figure>
          <img src="antmaze training loss.png" alt="AntMaze training loss curves for different encoders">
          <figcaption>Figure 1: Training loss curves for AntMaze-Medium task across different visual encoders. IMPALA shows rapid convergence to low loss values, while ResNet50 and DINOv3 exhibit slower convergence with higher variance. ViT-Small plateaus early at high loss, indicating underfitting.</figcaption>
        </figure>
        <figure>
          <img src="antmaze validation loss.png" alt="AntMaze validation loss curves for different encoders">
          <figcaption>Figure 2: Validation loss curves for AntMaze-Medium task. IMPALA maintains low validation loss throughout training, confirming its strong generalization. ResNet50 and DINOv3 show higher validation loss, while ViT-Small fails to reduce validation error effectively.</figcaption>
        </figure>
      </div>

      <p>
        One plausible explanation is that AntMaze requires a mixture of local and global reasoning: the agent needs to
        understand maze topology and wall locations but does not require extremely fine-grained texture or pose
        estimation. IMPALA's inductive bias and modest depth may strike the right balance here. In contrast, deeper
        networks and transformers might be over-parameterized for the dataset size, making them harder to train
        effectively in an offline regime without pretraining.
      </p>

      <p>
        The following videos illustrate the qualitative differences in navigation behavior between encoders. ResNet50
        demonstrates moderate navigation capability, successfully reaching the goal in some episodes but with less
        consistent path planning than IMPALA. In contrast, ViT-Small struggles significantly with navigation, often
        getting stuck or failing to make progress toward the goal, consistent with its low success rate (0.03).
      </p>

      <div class="figure-grid">
        <figure>
          <video controls>
            <source src="antmaze-resnet50.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption>Video 1: AntMaze-Medium navigation behavior with ResNet50 encoder. The agent demonstrates moderate navigation capability, successfully navigating through the maze in some episodes but with less consistent path planning compared to IMPALA. Success rate: 0.108.</figcaption>
        </figure>
        <figure>
          <video controls>
            <source src="antmaze-ViT.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption>Video 2: AntMaze-Medium navigation behavior with ViT-Small encoder. The agent struggles significantly with navigation, often getting stuck or failing to make progress toward the goal. This poor performance aligns with the low success rate (0.03) and the underfitting observed in the loss curves.</figcaption>
        </figure>
      </div>

      <h3>4.2 Visual Cube Manipulation</h3>
      <p>
        On the Visual Cube manipulation task, the ranking of encoders changes dramatically. Here, ResNet50 achieves the
        highest success rate (0.132), substantially outperforming IMPALA (0.02), the baseline, and the transformer-based encoders
        (both 0.00). This supports the hypothesis that deeper convolutional models are better suited to tasks that
        require precise geometric reasoning.
      </p>

      <p>
        Visual Cube demands accurate perception of cube orientation, edge alignment, and depth cues under frequent
        occlusions from the robot hand. Our qualitative analysis suggests that ResNet50's deeper feature hierarchy
        enables it to build more robust representations of cube pose than IMPALA, which appears to capture only coarse
        shape and position. The training and validation curves show that ResNet50 converges to a lower
        imitation loss on this task than other encoders, consistent with its superior performance.
      </p>

      <div class="figure-grid">
        <figure>
          <img src="single cube training loss.png" alt="Single Cube training loss curves for different encoders">
          <figcaption>Figure 3: Training loss curves for Visual Cube manipulation task. ResNet50 achieves the lowest training loss, demonstrating its superior capacity for learning fine-grained geometric representations. IMPALA converges to a higher loss, while transformer-based encoders (ViT-Small, DINOv3) fail to learn effectively.</figcaption>
        </figure>
        <figure>
          <img src="single cube validation loss.png" alt="Single Cube validation loss curves for different encoders">
          <figcaption>Figure 4: Validation loss curves for Visual Cube manipulation task. ResNet50's lower validation loss confirms its better generalization to unseen cube poses. The gap between ResNet50 and other encoders is more pronounced than in AntMaze, highlighting the importance of deeper feature hierarchies for precise manipulation tasks.</figcaption>
        </figure>
      </div>

      <p>
        Our ViT-Small and DINOv3-S/16 <a href="#ref2">[2]</a> both fail to solve the task, achieving zero success rate. This aligns with the intuition
        that compressing all spatial information into a single CLS token without explicit spatial inductive biases
        makes it difficult to preserve fine-grained pose information. Even though DINOv3 <a href="#ref2">[2]</a> is pretrained, its invariances
        to cropping, color jitter, and viewpoint are likely misaligned with the demands of low-level manipulation, where
        small visual differences correspond to large changes in the underlying state.
      </p>

      <p>
        The following videos demonstrate the qualitative differences in manipulation behavior across all four encoders.
        ResNet50 shows precise manipulation with accurate cube pose alignment, while IMPALA struggles with fine-grained
        control. Both transformer-based encoders (ViT-Small and DINOv3) fail to achieve successful manipulation,
        consistent with their zero success rates.
      </p>

      <div class="figure-grid">
        <figure>
          <video controls>
            <source src="single-cube-resnet50.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption>Video 3: Visual Cube manipulation with ResNet50 encoder. The agent demonstrates precise manipulation with accurate cube pose alignment, successfully matching the target orientation. This superior performance aligns with the highest success rate (0.132) among all encoders on this task.</figcaption>
        </figure>
        <figure>
          <video controls>
            <source src="single-cube-impala.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption>Video 4: Visual Cube manipulation with IMPALA encoder. The agent struggles with fine-grained control, capturing only coarse shape and position but failing to achieve precise cube orientation alignment. Success rate: 0.02.</figcaption>
        </figure>
        <figure>
          <video controls>
            <source src="single-cube-DinoV3.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption>Video 5: Visual Cube manipulation with DINOv3-S/16 encoder. The agent fails to achieve successful manipulation, unable to align the cube to the target pose. This failure demonstrates the misalignment between self-supervised pretraining invariances and the fine-grained geometric demands of manipulation. Success rate: 0.00.</figcaption>
        </figure>
        <figure>
          <video controls>
            <source src="single-cube-ViT.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption>Video 6: Visual Cube manipulation with ViT-Small encoder. The agent fails to solve the manipulation task, unable to preserve fine-grained pose information needed for precise control. This aligns with the zero success rate and the underfitting observed in training. Success rate: 0.00.</figcaption>
        </figure>
      </div>

      <h3>Insights</h3>
      <p>
        Comparing across both tasks, we see that:
      </p>
      <ul>
        <li>IMPALA excels at AntMaze but struggles on Visual Cube.</li>
        <li>ResNet50 shows the opposite pattern: weaker on AntMaze but clearly superior on Visual Cube.</li>
        <li>Transformer-based encoders (ViT-Small, DINOv3-S/16) perform poorly in both settings under our training
          protocol.</li>
      </ul>

      <h4>4.3.1 Task-Encoder Alignment</h4>
      <p>
        The stark contrast in encoder performance across tasks reveals a fundamental principle: <em>task-encoder alignment</em>
        matters more than raw capacity or pretraining status. AntMaze navigation requires understanding global spatial
        relationships‚Äîrecognizing walls, corridors, and open spaces‚Äîbut does not demand pixel-perfect precision. IMPALA's
        shallow convolutional architecture, with its strong locality bias and modest depth, captures these coarse geometric
        features efficiently. The training and validation loss curves (Figures 1‚Äì2) show that IMPALA converges rapidly
        to low loss values, indicating that its representation space is well-matched to the navigation task's requirements.
      </p>

      <p>
        In contrast, Visual Cube manipulation demands fine-grained geometric reasoning: the agent must perceive subtle
        differences in cube orientation, edge alignment, and depth cues under frequent occlusions. ResNet50's deeper
        feature hierarchy enables it to build hierarchical representations that capture both low-level edges and
        high-level pose information. The loss curves (Figures 3‚Äì4) reveal that ResNet50 achieves significantly lower
        training and validation loss on this task than IMPALA, confirming that its increased capacity is necessary and
        effectively utilized for precise manipulation.
      </p>

      <h4>4.3.2 The Capacity-Data Trade-off</h4>
      <p>
        Our results highlight a critical tension between model capacity and dataset size in offline RL. ResNet50's
        superior performance on Visual Cube suggests that deeper networks can learn more expressive representations when
        the task demands it. However, on AntMaze, where the task requirements are simpler, ResNet50's additional capacity
        appears to be a liability: it converges more slowly and achieves higher validation loss than IMPALA. This
        suggests that in offline settings with fixed datasets, over-parameterization can lead to worse generalization,
        especially when the task does not require the additional representational capacity.
      </p>

      <p>
        The transformer-based encoders (ViT-Small and DINOv3-S/16) exemplify this trade-off most dramatically. Despite
        ViT-Small having comparable parameter count to ResNet50, it fails to learn effective representations in both
        tasks. The loss curves show early plateauing at high loss values, indicating that the transformer architecture
        struggles to extract relevant features from the limited offline data. This aligns with the well-known data
        efficiency gap between CNNs and transformers: transformers require significantly more data to learn effective
        representations, which is particularly problematic in offline RL where data collection is expensive and the
        dataset is fixed.
      </p>

      <h4>4.3.3 Pretraining and Invariance Mismatch</h4>
      <p>
        DINOv3-S/16's failure on both tasks, despite being pretrained on large-scale image data, reveals a critical
        insight: <em>pretraining quality does not guarantee transfer to control</em>. DINOv3 was trained with
        self-supervised objectives that encourage invariances to cropping, color jitter, and viewpoint changes‚Äîfeatures
        that are beneficial for high-level vision tasks like classification and segmentation. However, these same
        invariances are detrimental for control tasks where small visual differences correspond to large changes in the
        underlying state. The videos (Videos 5‚Äì6) show that DINOv3 fails to achieve successful manipulation, unable to
        distinguish between subtly different cube poses that require different actions.
      </p>

      <p>
        This invariance mismatch is particularly pronounced in Visual Cube, where precise pose estimation is critical.
        Even though DINOv3's frozen features might capture rich semantic information, they lack the geometric precision
        needed for manipulation. The validation loss curves show that DINOv3 fails to reduce error effectively, suggesting
        that the pretrained features are fundamentally misaligned with the control objective, and that training only the
        policy head is insufficient to bridge this gap.
      </p>

      <h4>4.3.4 Architectural Inductive Biases</h4>
      <p>
        The performance differences between CNNs and transformers highlight the importance of architectural inductive
        biases. Convolutional networks, with their translation equivariance and locality bias, are naturally suited for
        processing spatial structure in images. This bias helps them learn robust representations even with limited data,
        as evidenced by IMPALA's strong performance on AntMaze and ResNet50's success on Visual Cube.
      </p>

      <p>
        Transformers, in contrast, rely on self-attention to learn spatial relationships from scratch. While this
        flexibility can be powerful with sufficient data, it becomes a liability in data-limited settings. The ViT-Small
        encoder's poor performance suggests that without explicit spatial inductive biases, the model struggles to learn
        the geometric structure needed for control. The early loss plateauing indicates that the attention mechanism
        fails to discover meaningful spatial patterns from the offline trajectories, leading to representation collapse
        or underfitting.
      </p>

      <h4>4.3.5 Implications for Offline GCRL</h4>
      <p>
        These insights have several important implications for offline goal-conditioned RL research. First, encoder
        choice should be treated as a first-class hyperparameter, not an afterthought. The dramatic performance
        differences we observe (e.g., ResNet50 achieving 0.132 success rate on Visual Cube versus IMPALA's 0.02)
        demonstrate that encoder selection can have a larger impact than algorithmic choices in some settings.
      </p>

      <p>
        Second, the task-encoder alignment principle suggests that researchers should carefully match encoder
        architectures to task requirements. For tasks requiring coarse spatial reasoning (like navigation), simpler
        architectures may be sufficient and more robust. For tasks requiring fine-grained geometric precision (like
        manipulation), deeper networks with stronger representational capacity may be necessary.
      </p>

      <p>
        Finally, our results caution against blindly importing state-of-the-art vision models into control pipelines.
        Pretrained models like DINOv3, despite their success in high-level vision tasks, may not transfer effectively to
        control without careful adaptation. Future work should explore task-specific adaptation strategies, such as
        lightweight fine-tuning, adapter layers, or hybrid architectures that combine the benefits of pretraining with
        the geometric precision needed for control.
      </p>

      <p>
        These observations support a nuanced view: encoder performance in offline GCRL depends not only
        on capacity or pretraining quality, but also on how inductive biases and invariances interact with the specific
        structure of the task. The "best" encoder is not universal‚Äîit depends on the task's perceptual demands, the
        available data, and the alignment between the encoder's representational capabilities and the control objective.
      </p>

      <h2>Discussion and Future Work</h2>
      <p>
        Our experiments show that visual encoders play a decisive role in offline goal-conditioned reinforcement learning
        and behavioral cloning. There is no single ‚Äúbest‚Äù encoder across tasks: shallow CNNs like IMPALA can outperform
        deeper models on navigation tasks, while deeper ResNets can be crucial for precise geometric manipulation. At the
        same time, transformer-based encoders‚Äîboth from scratch and pretrained‚Äîdo not automatically translate their
        success in high-level vision tasks into strong control performance under the constraints of OGBench.
      </p>

      <p>
        These results suggest several directions for future work. First, alternative pooling strategies for transformer
        features (e.g., spatial softmax, coordinate heads, or hybrid CNN‚ÄìViT architectures) might help preserve
        geometry without sacrificing the benefits of global attention. Second, lightweight task-specific adaptation of
        pretrained encoders, such as adapter layers or partial fine-tuning, could better align self-supervised features
        with control objectives while retaining the benefits of large-scale pretraining.
      </p>

      <p>
        A broader takeaway is methodological: encoder choice should be treated as a first-class design axis in control
        research, particularly in offline settings where the data distribution is fixed and representation learning is
        constrained. Benchmarks like OGBench are invaluable for algorithm comparison, but they also provide an
        opportunity to systematically investigate how representation learning affects control. We hope that this work
        encourages further studies at the intersection of visual encoding and offline goal-conditioned RL, and that
        future benchmarks will incorporate representation-focused baselines more explicitly.
      </p>
    </article>

    
    <article>
      <h2>Appendix</h2>

      <h3>A. Encoder Architecture Details</h3>
      <p>
        This section describes the encoder architectures used in our experiments in greater detail. All encoders operate
        on stacked RGB frames of shape \(H \times W \times 3k\) with \(k = 3\). Unless otherwise specified, we use
        \(H = W = 64\).
      </p>

      <h4>A.1 IMPALA</h4>
      <p>
        The IMPALA encoder follows the standard architecture used in many RL benchmarks:
      </p>
      <ul>
        <li>Three convolutional blocks with ReLU activations.</li>
        <li>Each block consists of a \(3 \times 3\) convolution with stride 2 followed by a \(3 \times 3\) convolution
          with stride 1.</li>
        <li>Channel sizes increase from 16 ‚Üí 32 ‚Üí 32 across blocks.</li>
        <li>The final feature map is flattened and passed through a linear layer to obtain a latent embedding
          \( z \in \mathbb{R}^{d} \), where \(d\) matches the policy input dimension used by OGBench.</li>
      </ul>

      <h4>A.2 ResNet50 (RL Variant)</h4>
      <p>
        Our ResNet50 backbone is adapted from the standard ImageNet configuration but modified for RL stability:
      </p>
      <ul>
        <li>All <strong>Batch Normalization</strong> layers are replaced by <strong>Group Normalization</strong> (GN)
          with 32 groups. This removes dependence on batch-level statistics and mitigates instability with correlated
          trajectory samples.</li>
        <li>The network uses an initial \(7 \times 7\) stride-2 convolution followed by a \(3 \times 3\) max-pooling
          layer, and four residual stages with bottleneck blocks and channel sizes 256, 512, 1024, and 2048.</li>
        <li>We apply global average pooling (GAP) over the final feature map to obtain a vector \(h \in \mathbb{R}^{2048}\)
          per frame.</li>
        <li>For frame stacks, the same ResNet is applied to each frame independently; the resulting vectors are
          concatenated to form a representation in \(\mathbb{R}^{2048 \times 3}\), which is then projected through a
          linear layer to the policy embedding dimension.</li>
      </ul>

      <h4>A.3 ViT-Small (from scratch)</h4>
      <p>
        The ViT-Small encoder is a compact Vision Transformer:
      </p>
      <ul>
        <li>Each image frame is split into non-overlapping \(16 \times 16\) patches.</li>
        <li>Patches are flattened and linearly projected to an embedding dimension \(D = 384\).</li>
        <li>A learnable [CLS] token is prepended to the sequence, and learnable positional embeddings are added to all tokens.</li>
        <li>The sequence is passed through 12 transformer encoder blocks, each with 6 attention heads, MLP dimension
          1536, LayerNorm, and residual connections.</li>
        <li>After the final block, we discard patch tokens and use the [CLS] token as the frame representation.</li>
        <li>Frames in the stack are processed in parallel (by reshaping the batch), and their [CLS] tokens are
          concatenated and linearly projected to the policy embedding dimension.</li>
      </ul>

      <h4>A.4 DINOv3-S/16 (Frozen)</h4>
      <p>
        For DINOv3-S/16, we use an off-the-shelf pretrained checkpoint provided by the original authors:
      </p>
      <ul>
        <li>Architecture is essentially identical to ViT-Small (patch size 16, embedding dimension 384, 12 blocks, 6
          heads).</li>
        <li>We use the [CLS] token as the global representation for each frame.</li>
        <li>Encoder weights are frozen: we stop gradients at the encoder output and only train the downstream policy
          head.</li>
      </ul>

      <h3>B. Training Procedure and Hyperparameters</h3>
      <p>
        This section lists the main hyperparameters used in our experiments.
      </p>
      <ul>
        <li><strong>Optimizer:</strong> Adam</li>
        <li><strong>Learning rate:</strong> \(3 \times 10^{-4}\)</li>
        <li><strong>Batch size:</strong> 256 transitions</li>
        <li><strong>Number of gradient steps:</strong> 500,000</li>
        <li><strong>Discount factor \(\gamma\):</strong> 0.99 (used only in environment reward computation, not in the
          supervised GCBC loss)</li>
        <li><strong>Frame stack:</strong> 3 consecutive frames</li>
        <li><strong>Data augmentation:</strong> random crop with probability 0.5 (4-pixel padding and crop back to
          \(64 \times 64\))</li>
        <li><strong>Activation function:</strong> GELU in the policy MLP, ReLU in CNNs, GELU in ViTs</li>
        <li><strong>Hardware:</strong> single NVIDIA GPU (e.g., RTX 3090) with JAX/Flax and XLA acceleration</li>
      </ul>

      <p>
        All experiments use the same random seed for fair comparison across encoder variants. We did not perform
        extensive hyperparameter tuning for individual encoders; rather, we chose a single GCBC configuration that was
        stable across all models.
      </p>

      <h3>C. Training and Validation Loss Curves</h3>
      <p>
        To better understand optimization dynamics, we track both training loss (on mini-batches sampled from the
        offline dataset) and validation loss (on a held-out subset of trajectories) over the course of training.
      </p>

      <p>
        Qualitatively, we observe the following trends:
      </p>
      <ul>
        <li><strong>IMPALA:</strong> training and validation losses decrease rapidly and plateau at relatively low
          values on both tasks, indicating that the model fits the data well without severe overfitting.</li>
        <li><strong>ResNet50:</strong> training loss decreases steadily, but validation loss flattens at a higher value
          on AntMaze than on Visual Cube, consistent with its weaker performance on navigation and stronger performance
          on manipulation.</li>
        <li><strong>ViT-Small:</strong> both training and validation losses decrease initially but plateau early at
          relatively high values, suggesting underfitting or difficulty in optimization.</li>
        <li><strong>DINOv3-S/16:</strong> validation loss decreases slightly but remains worse than ResNet50 and IMPALA
          on both tasks, consistent with the mismatch between self-supervised invariances and control objectives.</li>
      </ul>

      <p>
        The training and validation loss curves for both tasks are shown in Figures 1‚Äì4 in the Results section. These
        visualizations confirm the qualitative trends described above and provide insight into the optimization dynamics
        of each encoder architecture.
      </p>
      
    </article>

    <h2>References</h2>
    <ol style="line-height:1.6">

      <li id="ref1">
        A. Krizhevsky, I. Sutskever, and G. E. Hinton.
        <strong>ImageNet classification with deep convolutional neural networks.</strong>
        In <em>Proceedings of the 26th International Conference on Neural Information Processing Systems</em>, 
        pages 1097‚Äì1105, 2012.
      </li>

      <li id="ref2">
        O. Sim√©oni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov,
        M. Szafraniec, S. Yi, M. Ramamonjisoa, F. Massa, D. Haziza, L. Wehrstedt,
        J. Wang, T. Darcet, T. Moutakanni, L. Sentana, C. Roberts, A. Vedaldi, 
        J. Tolan, J. Brandt, C. Couprie, J. Mairal, H. J√©gou, P. Labatut, and P. Bojanowski.
        <strong>DINOv3.</strong>
        <em>arXiv preprint arXiv:2508.10104</em>, 2025.
      </li>

      <li id="ref3">
        S. Park, K. Frans, B. Eysenbach, and S. Levine.
        <strong>OGBench: Benchmarking offline goal-conditioned RL.</strong>
        <em>arXiv preprint arXiv:2410.20092</em>, 2025.
      </li>

      <li>
        N. Rudin, D. Hoeller, P. Reist, and M. Hutter.
        <strong>Learning to walk in minutes using massively parallel deep reinforcement learning.</strong>
        <em>arXiv preprint arXiv:2109.11978</em>, 2021.
      </li>

      <li>
        D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen,
        E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine.
        <strong>QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation.</strong>
        <em>arXiv preprint arXiv:1806.10293</em>, 2018.
      </li>

      <li>
        OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. Leoni Aleman,
        D. Almeida, J. Altenschmidt, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji,
        V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine,
        G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd,
        and many others.
        <strong>GPT-4 technical report.</strong>
        <em>arXiv preprint arXiv:2303.08774</em>, 2024.
      </li>

    </ol>
  </div>
</body>
</html>